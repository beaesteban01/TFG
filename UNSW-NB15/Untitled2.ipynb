{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0      1              2     3    4    5         6    7    8   9   \\\n",
      "0  59.166.0.0   1390  149.171.126.6    53  udp  CON  0.001055  132  164  31   \n",
      "1  59.166.0.0  33661  149.171.126.9  1024  udp  CON  0.036133  528  304  31   \n",
      "2  59.166.0.6   1464  149.171.126.7    53  udp  CON  0.001119  146  178  31   \n",
      "\n",
      "   ...  39  40  41 42  43  44  45  46   47  48  \n",
      "0  ...   0   3   7  1   3   1   1   1  NaN   0  \n",
      "1  ...   0   2   4  2   3   1   1   2  NaN   0  \n",
      "2  ...   0  12   8  1   2   2   1   1  NaN   0  \n",
      "\n",
      "[3 rows x 49 columns]\n",
      "Read 700001 rows.\n",
      "Train on 525000 samples, validate on 175001 samples\n",
      "Epoch 1/1000\n",
      " - 22s - loss: 0.0213 - acc: 0.9864 - val_loss: 6.5535e-04 - val_acc: 0.9999\n",
      "Epoch 2/1000\n",
      " - 23s - loss: 4.9873e-04 - acc: 0.9999 - val_loss: 2.3873e-04 - val_acc: 1.0000\n",
      "Epoch 3/1000\n",
      " - 25s - loss: 2.7308e-04 - acc: 0.9999 - val_loss: 2.3436e-05 - val_acc: 1.0000\n",
      "Epoch 4/1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "\n",
    "#from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "from inspect import signature\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"UNSW-NB15_1.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "df = pd.read_csv(path, header=None)\n",
    "print(df[0:3])\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "#df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'srcip',\n",
    "    'sport',\n",
    "    'dstip',\n",
    "    'dsport',\n",
    "    'proto',\n",
    "    'state',\n",
    "    'dur',\n",
    "    'sbyte',\n",
    "    'dbytes',\n",
    "    'sttl',\n",
    "    'dttl',\n",
    "    'sloss',\n",
    "    'dloss',\n",
    "    'service',\n",
    "    'Sload',\n",
    "    'Dload',\n",
    "    'Spkts',\n",
    "    'Dpkts',\n",
    "    'swin',\n",
    "    'dwin',\n",
    "    'stcpb',\n",
    "    'dtcpb',\n",
    "    'smeansz',\n",
    "    'dmeansz',\n",
    "    'trans',\n",
    "    'res',\n",
    "    'Sjit',\n",
    "    'Djit',\n",
    "    'Stime', #QUITAR\n",
    "    'Ltime', #QUITAR\n",
    "    'Sintpkt',\n",
    "    'Dintpkt',\n",
    "    'tcprtt',\n",
    "    'synack',\n",
    "    'ackdat',\n",
    "    'is_sm_ips_ports',\n",
    "    'ct_state_ttl',\n",
    "    'ct_flw_http_mthd',\n",
    "    'is_ftp_login',\n",
    "    'ct_ftp_cmd',\n",
    "    'ct_srv_src',\n",
    "    'ct_srv_dst',\n",
    "    'ct_dst_ltm',\n",
    "    'ct_src_ltm', \n",
    "    'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_lt', \n",
    "    'ct_dst_src_ltm',\n",
    "    'attack_cat',\n",
    "    'Label'\n",
    "]\n",
    "\n",
    "df.drop('sport', 1, inplace=True)#\n",
    "df.drop('dsport', 1, inplace=True)#\n",
    "df.drop('state', 1, inplace=True)#\n",
    "df.drop('dur', 1, inplace=True)#\n",
    "df.drop('Sload', 1, inplace=True)#\n",
    "df.drop('Dload', 1, inplace=True)#\n",
    "df.drop('Stime', 1, inplace=True)#\n",
    "df.drop('Ltime', 1, inplace=True)#\n",
    "df.drop('ct_src_ltm', 1, inplace=True)#\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "#analyze(path)\n",
    "\n",
    "\n",
    "# display 5 rows\n",
    "df[0:5]\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "#Clean 'date' column and convert to Int type\n",
    "def clean_date(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_removed = s.replace(\" \", \"\")\n",
    "    s_int = int(s_removed)\n",
    "    return s_int\n",
    "\n",
    "########## CLEAN IP #######################\n",
    "def clean_ip(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_int = int(s)\n",
    "    return s_int\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "#MINMAX -1 1\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def min_max_1(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "#MINMAX 0 1\n",
    "def min_max_0(df, name, normalized_low=0, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "    \n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['srcip'] = df['srcip'].apply(clean_ip)\n",
    "df['dstip'] = df['dstip'].apply(clean_ip)\n",
    "\n",
    "# Now encode the feature vector\n",
    "\n",
    "encode_numeric_zscore(df, 'srcip')#\n",
    "#encode_numeric_zscore((df, 'sport')\n",
    "\n",
    "encode_numeric_zscore(df, 'dstip') #\n",
    "#encode_numeric_zscore(df, 'dsport') #must be str not int ???\n",
    "\n",
    "encode_text_dummy(df, 'proto') #\n",
    "#encode_text_dummy(df, 'state')\n",
    "\n",
    "#encode_numeric_zscore(df, 'dur')\n",
    "\n",
    "encode_numeric_zscore(df, 'sbyte') #\n",
    "encode_numeric_zscore(df, 'dbytes')#\n",
    "encode_numeric_zscore(df, 'sttl')#\n",
    "encode_numeric_zscore(df, 'dttl')#\n",
    "encode_numeric_zscore(df, 'sloss')#\n",
    "encode_numeric_zscore(df, 'dloss')#\n",
    "encode_text_dummy(df, 'service') #\n",
    "#encode_numeric_zscore(df, 'Sload')\n",
    "\n",
    "#encode_numeric_zscore(df, 'Dload')\n",
    "\n",
    "encode_numeric_zscore(df, 'Spkts')#\n",
    "encode_numeric_zscore(df, 'Dpkts')#\n",
    "encode_numeric_zscore(df, 'swin')#\n",
    "encode_numeric_zscore(df, 'dwin')#\n",
    "encode_numeric_zscore(df, 'stcpb')#\n",
    "encode_numeric_zscore(df, 'dtcpb')#\n",
    "encode_numeric_zscore(df, 'smeansz')#\n",
    "encode_numeric_zscore(df, 'dmeansz')#\n",
    "encode_numeric_zscore(df, 'trans')#\n",
    "encode_numeric_zscore(df, 'res')#\n",
    "encode_numeric_zscore(df, 'Sjit')#\n",
    "encode_numeric_zscore(df, 'Djit')#\n",
    "\n",
    "encode_numeric_zscore(df, 'Sintpkt')#\n",
    "encode_numeric_zscore(df, 'Dintpkt')#\n",
    "encode_numeric_zscore(df, 'tcprtt')#\n",
    "encode_numeric_zscore(df, 'synack')#\n",
    "encode_numeric_zscore(df, 'ackdat')#\n",
    "#encode_numeric_zscore(df, 'is_sm_ips_ports')#\n",
    "encode_numeric_zscore(df, 'ct_state_ttl')#\n",
    "encode_numeric_zscore(df, 'ct_flw_http_mthd')#\n",
    "#encode_numeric_zscore(df, 'is_ftp_login')#\n",
    "encode_numeric_zscore(df, 'ct_ftp_cmd')#\n",
    "encode_numeric_zscore(df, 'ct_srv_src')#\n",
    "encode_numeric_zscore(df, 'ct_srv_dst')#\n",
    "encode_numeric_zscore(df, 'ct_dst_ltm')#\n",
    "#encode_numeric_zscore(df, 'ct_src_ltm')\n",
    "\n",
    "encode_numeric_zscore(df, 'ct_src_dport_ltm')#\n",
    "encode_numeric_zscore(df, 'ct_dst_sport_lt')#\n",
    "encode_numeric_zscore(df, 'ct_dst_src_ltm')#\n",
    "#encode_numeric_zscore(df, 'attack_cat')#\n",
    "encode_text_dummy(df, 'attack_cat')\n",
    "\n",
    "outcomes = encode_text_index(df, 'Label')#\n",
    "num_classes = len(outcomes)\n",
    "\n",
    "# display 5 rows\n",
    "\n",
    "df.dropna(inplace=True,axis=1)\n",
    "df[0:5]\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Break into X (predictors) & y (prediction)\n",
    "x, y = to_xy(df,'Label')\n",
    "#################################################\n",
    "\n",
    "\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, verbose=1, mode='auto')\n",
    "history = model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "\n",
    "\n",
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))\n",
    "\n",
    "print(outcomes)\n",
    "\n",
    "### PLOT ACCURACY ####\n",
    "\n",
    "plt.plot(np.arange(len(history.history['acc'])),\n",
    "history.history['acc'], label='training')\n",
    "plt.plot(np.arange(len(history.history['val_acc'])),\n",
    "history.history['val_acc'], label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy ')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "### PLOT CONFUSION MATRIX ###\n",
    "\n",
    "# Not normalized\n",
    "cm = confusion_matrix(y_eval, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, outcomes)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, outcomes, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### PLOT ROC ###\n",
    "\n",
    "#Plot an ROC. pred - the predictions, y - the expected outpus.\n",
    "#En mi caso creo que es pred == pred y== y_eval\n",
    "def plot_roc(pred, y):\n",
    "    fpr, tpr, _ = roc_curve(y, pred)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc(pred, y_eval)\n",
    "\n",
    "\n",
    "### PRECISION-RECALL ###\n",
    "\n",
    "average_precision = average_precision_score(y_eval, pred)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_eval, pred)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tf_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
