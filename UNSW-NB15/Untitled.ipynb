{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-87b2e6743d82>, line 219)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-87b2e6743d82>\"\u001b[0;36m, line \u001b[0;32m219\u001b[0m\n\u001b[0;31m    df.drop('sport', 1, inplace=True)#\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"UNSW-NB15.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "df = pd.read_csv(path, header=None)\n",
    "print(df[0:3])\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'srcip',\n",
    "    'sport',\n",
    "    'dstip',\n",
    "    'dsport',\n",
    "    'proto',\n",
    "    'state',\n",
    "    'sbyte',\n",
    "    'dbytes',\n",
    "    'sttl',\n",
    "    'dttl',\n",
    "    'sloss',\n",
    "    'dloss',\n",
    "    'service',\n",
    "    'Sload',\n",
    "    'Dload',\n",
    "    'Spkts',\n",
    "    'Dpkts',\n",
    "    'swin',\n",
    "    'dwin',\n",
    "    'stcpb',\n",
    "    'dtcpb',\n",
    "    'smeansz',\n",
    "    'dmeansz',\n",
    "    'trans',\n",
    "    'res',\n",
    "    'Sjit',\n",
    "    'Djit',\n",
    "    'Stime', #QUITAR\n",
    "    'Ltime', #QUITAR\n",
    "    'Sintpkt',\n",
    "    'Dintpkt',\n",
    "    'tcprtt',\n",
    "    'synack',\n",
    "    'ackdat',\n",
    "    'is_sm_ips_ports',\n",
    "    'ct_state_ttl',\n",
    "    'ct_flw_http_mthd',\n",
    "    'is_ftp_login',\n",
    "    'ct_ftp_cmd',\n",
    "    'ct_srv_src',\n",
    "    'ct_srv_dst',\n",
    "    'ct_dst_ltm',\n",
    "    'ct_src_ltm', \n",
    "    'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_lt', \n",
    "    'ct_dst_src_ltm',\n",
    "    'attack_cat',\n",
    "    'Label'\n",
    "\n",
    "]\n",
    "\n",
    "#df.drop('algo_borrar', 1, inplace=True)\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "analyze(path)\n",
    "\n",
    "\n",
    "# display 5 rows\n",
    "df[0:5]\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "#Clean 'date' column and convert to Int type\n",
    "def clean_date(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_removed = s.replace(\" \", \"\")\n",
    "    s_int = int(s_removed)\n",
    "    return s_int\n",
    "\n",
    "########## CLEAN IP #######################\n",
    "def clean_ip(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_int = int(s)\n",
    "    return s_int\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "#MINMAX -1 1\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def min_max_1(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "#MINMAX 0 1\n",
    "def min_max_0(df, name, normalized_low=0, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "    \n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['srcip'] = df['srcip'].apply(clean_ip)\n",
    "df['dstip'] = df['dstip'].apply(clean_ip)\n",
    "\n",
    "# Now encode the feature vector\n",
    "\n",
    "encode_numeric_zscore((df, 'srcip')#\n",
    "#encode_numeric_zscore((df, 'sport')\n",
    "df.drop('sport', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'dstip') #\n",
    "#encode_numeric_zscore(df, 'dsport') #must be str not int ???\n",
    "df.drop('dsport', 1, inplace=True)#\n",
    "encode_text_dummy(df, 'proto') #\n",
    "#encode_text_dummy(df, 'state')\n",
    "df.drop('state', 1, inplace=True)#\n",
    "#encode_numeric_zscore(df, 'dur')\n",
    "df.drop('dur', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'sbyte') #\n",
    "encode_numeric_zscore(df, 'dbytes')#\n",
    "encode_numeric_zscore(df, 'sttl')#\n",
    "encode_numeric_zscore(df, 'dttl')#\n",
    "encode_numeric_zscore(df, 'sloss')#\n",
    "encode_numeric_zscore(df, 'dloss')#\n",
    "encode_text_dummy(df, 'service') #\n",
    "#encode_numeric_zscore(df, 'Sload')\n",
    "df.drop('Sload', 1, inplace=True)#\n",
    "#encode_numeric_zscore(df, 'Dload')\n",
    "df.drop('Dload', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'Spkts')#\n",
    "encode_numeric_zscore(df, 'Dpkts')#\n",
    "encode_numeric_zscore(df, 'swin')#\n",
    "encode_numeric_zscore(df, 'dwin')#\n",
    "encode_numeric_zscore(df, 'stcpb')#\n",
    "encode_numeric_zscore(df, 'dtcpb')#\n",
    "encode_numeric_zscore(df, 'smeansz')#\n",
    "encode_numeric_zscore(df, 'dmeansz')#\n",
    "encode_numeric_zscore(df, 'trans')#\n",
    "encode_numeric_zscore(df, 'res')#\n",
    "encode_numeric_zscore(df, 'Sjit')#\n",
    "encode_numeric_zscore(df, 'Djit')#\n",
    "df.drop('Stime', 1, inplace=True)#\n",
    "df.drop('Ltime', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'Sintpkt')#\n",
    "encode_numeric_zscore(df, 'Dintpkt')#\n",
    "encode_numeric_zscore(df, 'tcprtt')#\n",
    "encode_numeric_zscore(df, 'synack')#\n",
    "encode_numeric_zscore(df, 'ackdat')#\n",
    "#encode_numeric_zscore(df, 'is_sm_ips_ports')#\n",
    "encode_numeric_zscore(df, 'ct_state_ttl')#\n",
    "encode_numeric_zscore(df, 'ct_flw_http_mthd')#\n",
    "#encode_numeric_zscore(df, 'is_ftp_login')#\n",
    "encode_numeric_zscore(df, 'ct_ftp_cmd')#\n",
    "encode_numeric_zscore(df, 'ct_srv_src')#\n",
    "encode_numeric_zscore(df, 'ct_srv_dst')#\n",
    "encode_numeric_zscore(df, 'ct_dst_ltm')#\n",
    "#encode_numeric_zscore(df, 'ct_src_ltm')\n",
    "df.drop('ct_src_ltm', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'ct_src_dport_ltm')#\n",
    "encode_numeric_zscore(df, 'ct_dst_sport_lt')#\n",
    "encode_numeric_zscore(df, 'ct_dst_src_ltm')#\n",
    "encode_numeric_zscore(df, 'attack_cat')#\n",
    "\n",
    "\n",
    "outcomes = encode_text_index(df, 'Label')#\n",
    "num_classes = len(outcomes)\n",
    "\n",
    "# display 5 rows\n",
    "\n",
    "df.dropna(inplace=True,axis=1)\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-516f7528e7be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sport'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df['sport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0      1              2     3    4    5         6    7    8   9   \\\n",
      "0  59.166.0.0   1390  149.171.126.6    53  udp  CON  0.001055  132  164  31   \n",
      "1  59.166.0.0  33661  149.171.126.9  1024  udp  CON  0.036133  528  304  31   \n",
      "2  59.166.0.6   1464  149.171.126.7    53  udp  CON  0.001119  146  178  31   \n",
      "\n",
      "   ...  39  40  41 42  43  44  45  46   47  48  \n",
      "0  ...   0   3   7  1   3   1   1   1  NaN   0  \n",
      "1  ...   0   2   4  2   3   1   1   2  NaN   0  \n",
      "2  ...   0  12   8  1   2   2   1   1  NaN   0  \n",
      "\n",
      "[3 rows x 49 columns]\n",
      "Read 700001 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"UNSW-NB15_1.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "df = pd.read_csv(path, header=None)\n",
    "print(df[0:3])\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'srcip',\n",
    "    'sport',\n",
    "    'dstip',\n",
    "    'dsport',\n",
    "    'proto',\n",
    "    'state',\n",
    "    'sbyte',\n",
    "    'dbytes',\n",
    "    'sttl',\n",
    "    'dttl',\n",
    "    'sloss',\n",
    "    'dloss',\n",
    "    'service',\n",
    "    'Sload',\n",
    "    'Dload',\n",
    "    'Spkts',\n",
    "    'Dpkts',\n",
    "    'swin',\n",
    "    'dwin',\n",
    "    'stcpb',\n",
    "    'dtcpb',\n",
    "    'smeansz',\n",
    "    'dmeansz',\n",
    "    'trans',\n",
    "    'res',\n",
    "    'Sjit',\n",
    "    'Djit',\n",
    "    'Stime', #QUITAR\n",
    "    'Ltime', #QUITAR\n",
    "    'Sintpkt',\n",
    "    'Dintpkt',\n",
    "    'tcprtt',\n",
    "    'synack',\n",
    "    'ackdat',\n",
    "    'is_sm_ips_ports',\n",
    "    'ct_state_ttl',\n",
    "    'ct_flw_http_mthd',\n",
    "    'is_ftp_login',\n",
    "    'ct_ftp_cmd',\n",
    "    'ct_srv_src',\n",
    "    'ct_srv_dst',\n",
    "    'ct_dst_ltm',\n",
    "    'ct_src_ltm', \n",
    "    'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_lt', \n",
    "    'ct_dst_src_ltm',\n",
    "    'attack_cat',\n",
    "    'Label'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/beatrizesteban/Desktop/csv_tfg/UNSW-NB15'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-1ea103224931>, line 134)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1ea103224931>\"\u001b[0;36m, line \u001b[0;32m134\u001b[0m\n\u001b[0;31m    df.drop('sport', 1, inplace=True)#\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ENCODING = 'utf-8'\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "analyze(path)\n",
    "\n",
    "\n",
    "# display 5 rows\n",
    "df[0:5]\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "#Clean 'date' column and convert to Int type\n",
    "def clean_date(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_removed = s.replace(\" \", \"\")\n",
    "    s_int = int(s_removed)\n",
    "    return s_int\n",
    "\n",
    "########## CLEAN IP #######################\n",
    "def clean_ip(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_int = int(s)\n",
    "    return s_int\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "#MINMAX -1 1\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def min_max_1(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "#MINMAX 0 1\n",
    "def min_max_0(df, name, normalized_low=0, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "        * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "    \n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['srcip'] = df['srcip'].apply(clean_ip)\n",
    "df['dstip'] = df['dstip'].apply(clean_ip)\n",
    "\n",
    "# Now encode the feature vector\n",
    "\n",
    "encode_numeric_zscore((df, 'srcip')#\n",
    "#encode_numeric_zscore((df, 'sport')\n",
    "df.drop('sport', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'dstip') #\n",
    "#encode_numeric_zscore(df, 'dsport') #must be str not int ???\n",
    "df.drop('dsport', 1, inplace=True)#\n",
    "encode_text_dummy(df, 'proto') #\n",
    "#encode_text_dummy(df, 'state')\n",
    "df.drop('state', 1, inplace=True)#\n",
    "#encode_numeric_zscore(df, 'dur')\n",
    "df.drop('dur', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'sbyte') #\n",
    "encode_numeric_zscore(df, 'dbytes')#\n",
    "encode_numeric_zscore(df, 'sttl')#\n",
    "encode_numeric_zscore(df, 'dttl')#\n",
    "encode_numeric_zscore(df, 'sloss')#\n",
    "encode_numeric_zscore(df, 'dloss')#\n",
    "encode_text_dummy(df, 'service') #\n",
    "#encode_numeric_zscore(df, 'Sload')\n",
    "df.drop('Sload', 1, inplace=True)#\n",
    "#encode_numeric_zscore(df, 'Dload')\n",
    "df.drop('Dload', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'Spkts')#\n",
    "encode_numeric_zscore(df, 'Dpkts')#\n",
    "encode_numeric_zscore(df, 'swin')#\n",
    "encode_numeric_zscore(df, 'dwin')#\n",
    "encode_numeric_zscore(df, 'stcpb')#\n",
    "encode_numeric_zscore(df, 'dtcpb')#\n",
    "encode_numeric_zscore(df, 'smeansz')#\n",
    "encode_numeric_zscore(df, 'dmeansz')#\n",
    "encode_numeric_zscore(df, 'trans')#\n",
    "encode_numeric_zscore(df, 'res')#\n",
    "encode_numeric_zscore(df, 'Sjit')#\n",
    "encode_numeric_zscore(df, 'Djit')#\n",
    "df.drop('Stime', 1, inplace=True)#\n",
    "df.drop('Ltime', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'Sintpkt')#\n",
    "encode_numeric_zscore(df, 'Dintpkt')#\n",
    "encode_numeric_zscore(df, 'tcprtt')#\n",
    "encode_numeric_zscore(df, 'synack')#\n",
    "encode_numeric_zscore(df, 'ackdat')#\n",
    "#encode_numeric_zscore(df, 'is_sm_ips_ports')#\n",
    "encode_numeric_zscore(df, 'ct_state_ttl')#\n",
    "encode_numeric_zscore(df, 'ct_flw_http_mthd')#\n",
    "#encode_numeric_zscore(df, 'is_ftp_login')#\n",
    "encode_numeric_zscore(df, 'ct_ftp_cmd')#\n",
    "encode_numeric_zscore(df, 'ct_srv_src')#\n",
    "encode_numeric_zscore(df, 'ct_srv_dst')#\n",
    "encode_numeric_zscore(df, 'ct_dst_ltm')#\n",
    "#encode_numeric_zscore(df, 'ct_src_ltm')\n",
    "df.drop('ct_src_ltm', 1, inplace=True)#\n",
    "encode_numeric_zscore(df, 'ct_src_dport_ltm')#\n",
    "encode_numeric_zscore(df, 'ct_dst_sport_lt')#\n",
    "encode_numeric_zscore(df, 'ct_dst_src_ltm')#\n",
    "encode_numeric_zscore(df, 'attack_cat')#\n",
    "\n",
    "\n",
    "outcomes = encode_text_index(df, 'Label')#\n",
    "num_classes = len(outcomes)\n",
    "\n",
    "# display 5 rows\n",
    "\n",
    "df.dropna(inplace=True,axis=1)\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           59.166.0.0\n",
      "1           59.166.0.0\n",
      "2           59.166.0.6\n",
      "3           59.166.0.5\n",
      "4           59.166.0.3\n",
      "5           59.166.0.0\n",
      "6           59.166.0.6\n",
      "7          10.40.182.3\n",
      "8           59.166.0.5\n",
      "9           59.166.0.7\n",
      "10         10.40.170.2\n",
      "11         10.40.170.2\n",
      "12         10.40.182.3\n",
      "13          59.166.0.1\n",
      "14          59.166.0.1\n",
      "15          59.166.0.1\n",
      "16          59.166.0.2\n",
      "17          59.166.0.1\n",
      "18          59.166.0.1\n",
      "19          59.166.0.4\n",
      "20        175.45.176.3\n",
      "21        175.45.176.2\n",
      "22        175.45.176.0\n",
      "23          59.166.0.3\n",
      "24          59.166.0.2\n",
      "25          59.166.0.8\n",
      "26          59.166.0.9\n",
      "27          59.166.0.0\n",
      "28          59.166.0.5\n",
      "29          59.166.0.7\n",
      "              ...     \n",
      "699971      59.166.0.1\n",
      "699972      59.166.0.7\n",
      "699973      59.166.0.9\n",
      "699974      59.166.0.0\n",
      "699975      59.166.0.3\n",
      "699976      59.166.0.2\n",
      "699977      59.166.0.3\n",
      "699978      59.166.0.3\n",
      "699979      59.166.0.8\n",
      "699980      59.166.0.4\n",
      "699981      59.166.0.1\n",
      "699982      59.166.0.2\n",
      "699983      59.166.0.8\n",
      "699984      59.166.0.3\n",
      "699985      59.166.0.0\n",
      "699986      59.166.0.3\n",
      "699987      59.166.0.6\n",
      "699988      59.166.0.6\n",
      "699989      59.166.0.6\n",
      "699990      59.166.0.4\n",
      "699991      59.166.0.8\n",
      "699992      59.166.0.3\n",
      "699993      59.166.0.3\n",
      "699994      59.166.0.3\n",
      "699995      59.166.0.6\n",
      "699996      59.166.0.8\n",
      "699997      59.166.0.0\n",
      "699998      59.166.0.0\n",
      "699999      59.166.0.6\n",
      "700000      59.166.0.0\n",
      "Name: srcip, Length: 700001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['srcip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          1390\n",
      "1         33661\n",
      "2          1464\n",
      "3          3593\n",
      "4         49664\n",
      "5         32119\n",
      "6          2142\n",
      "7             0\n",
      "8         40726\n",
      "9         12660\n",
      "10            0\n",
      "11            0\n",
      "12            0\n",
      "13        48847\n",
      "14        24266\n",
      "15        10393\n",
      "16        62539\n",
      "17        21270\n",
      "18         8989\n",
      "19        49346\n",
      "20        21223\n",
      "21        23357\n",
      "22        13284\n",
      "23         4192\n",
      "24        26872\n",
      "25        24946\n",
      "26         5685\n",
      "27        22848\n",
      "28        28565\n",
      "29        46719\n",
      "          ...  \n",
      "699971    46807\n",
      "699972    60323\n",
      "699973    65260\n",
      "699974    36648\n",
      "699975    14404\n",
      "699976    60849\n",
      "699977    14472\n",
      "699978    18827\n",
      "699979    24528\n",
      "699980    19461\n",
      "699981    35327\n",
      "699982     9969\n",
      "699983    33100\n",
      "699984    12691\n",
      "699985    18895\n",
      "699986    21661\n",
      "699987    10213\n",
      "699988    31786\n",
      "699989    52262\n",
      "699990    28517\n",
      "699991    34947\n",
      "699992    48114\n",
      "699993    48812\n",
      "699994    48114\n",
      "699995     8543\n",
      "699996    12520\n",
      "699997    18895\n",
      "699998    30103\n",
      "699999    30388\n",
      "700000     6055\n",
      "Name: sport, Length: 700001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['sport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('sport', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0      1              2     3    4    5         6    7    8   9   \\\n",
      "0  59.166.0.0   1390  149.171.126.6    53  udp  CON  0.001055  132  164  31   \n",
      "1  59.166.0.0  33661  149.171.126.9  1024  udp  CON  0.036133  528  304  31   \n",
      "2  59.166.0.6   1464  149.171.126.7    53  udp  CON  0.001119  146  178  31   \n",
      "\n",
      "   ...  39  40  41 42  43  44  45  46   47  48  \n",
      "0  ...   0   3   7  1   3   1   1   1  NaN   0  \n",
      "1  ...   0   2   4  2   3   1   1   2  NaN   0  \n",
      "2  ...   0  12   8  1   2   2   1   1  NaN   0  \n",
      "\n",
      "[3 rows x 49 columns]\n",
      "Read 700001 rows.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['dur'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b3ceb5d6c40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dsport'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dur'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['dur'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"UNSW-NB15_1.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "df = pd.read_csv(path, header=None)\n",
    "print(df[0:3])\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'srcip',\n",
    "    'sport',\n",
    "    'dstip',\n",
    "    'dsport',\n",
    "    'proto',\n",
    "    'state',\n",
    "    'sbyte',\n",
    "    'dbytes',\n",
    "    'sttl',\n",
    "    'dttl',\n",
    "    'sloss',\n",
    "    'dloss',\n",
    "    'service',\n",
    "    'Sload',\n",
    "    'Dload',\n",
    "    'Spkts',\n",
    "    'Dpkts',\n",
    "    'swin',\n",
    "    'dwin',\n",
    "    'stcpb',\n",
    "    'dtcpb',\n",
    "    'smeansz',\n",
    "    'dmeansz',\n",
    "    'trans',\n",
    "    'res',\n",
    "    'Sjit',\n",
    "    'Djit',\n",
    "    'Stime', #QUITAR\n",
    "    'Ltime', #QUITAR\n",
    "    'Sintpkt',\n",
    "    'Dintpkt',\n",
    "    'tcprtt',\n",
    "    'synack',\n",
    "    'ackdat',\n",
    "    'is_sm_ips_ports',\n",
    "    'ct_state_ttl',\n",
    "    'ct_flw_http_mthd',\n",
    "    'is_ftp_login',\n",
    "    'ct_ftp_cmd',\n",
    "    'ct_srv_src',\n",
    "    'ct_srv_dst',\n",
    "    'ct_dst_ltm',\n",
    "    'ct_src_ltm', \n",
    "    'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_lt', \n",
    "    'ct_dst_src_ltm',\n",
    "    'attack_cat',\n",
    "    'Label'\n",
    "\n",
    "]\n",
    "\n",
    "df.drop('sport', 1, inplace=True)#\n",
    "df.drop('dsport', 1, inplace=True)#\n",
    "df.drop('state', 1, inplace=True)#\n",
    "df.drop('dur', 1, inplace=True)#\n",
    "df.drop('Sload', 1, inplace=True)#\n",
    "df.drop('Dload', 1, inplace=True)#\n",
    "df.drop('Stime', 1, inplace=True)#\n",
    "df.drop('Ltime', 1, inplace=True)#\n",
    "df.drop('ct_src_ltm', 1, inplace=True)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0      1              2     3    4    5         6    7    8   9   \\\n",
      "0  59.166.0.0   1390  149.171.126.6    53  udp  CON  0.001055  132  164  31   \n",
      "1  59.166.0.0  33661  149.171.126.9  1024  udp  CON  0.036133  528  304  31   \n",
      "2  59.166.0.6   1464  149.171.126.7    53  udp  CON  0.001119  146  178  31   \n",
      "\n",
      "   ...  39  40  41 42  43  44  45  46   47  48  \n",
      "0  ...   0   3   7  1   3   1   1   1  NaN   0  \n",
      "1  ...   0   2   4  2   3   1   1   2  NaN   0  \n",
      "2  ...   0  12   8  1   2   2   1   1  NaN   0  \n",
      "\n",
      "[3 rows x 49 columns]\n",
      "Read 700001 rows.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 48 elements, new values have 49 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4185d833e58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m'ct_dst_src_ltm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m'attack_cat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;34m'Label'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m ]\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5079\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5080\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5082\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n\u001b[1;32m    154\u001b[0m                 \u001b[0;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 48 elements, new values have 49 elements"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"UNSW-NB15_1.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "df = pd.read_csv(path, header=None)\n",
    "print(df[0:3])\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'srcip',\n",
    "    'sport',\n",
    "    'dstip',\n",
    "    'dsport',\n",
    "    'proto',\n",
    "    'state',\n",
    "    'dur',\n",
    "    'sbyte',\n",
    "    'dbytes',\n",
    "    'sttl',\n",
    "    'dttl',\n",
    "    'sloss',\n",
    "    'dloss',\n",
    "    'service',\n",
    "    'Sload',\n",
    "    'Dload',\n",
    "    'Spkts',\n",
    "    'Dpkts',\n",
    "    'swin',\n",
    "    'dwin',\n",
    "    'stcpb',\n",
    "    'dtcpb',\n",
    "    'smeansz',\n",
    "    'dmeansz',\n",
    "    'trans',\n",
    "    'res',\n",
    "    'Sjit',\n",
    "    'Djit',\n",
    "    'Stime', #QUITAR\n",
    "    'Ltime', #QUITAR\n",
    "    'Sintpkt',\n",
    "    'Dintpkt',\n",
    "    'tcprtt',\n",
    "    'synack',\n",
    "    'ackdat',\n",
    "    'is_sm_ips_ports',\n",
    "    'ct_state_ttl',\n",
    "    'ct_flw_http_mthd',\n",
    "    'is_ftp_login',\n",
    "    'ct_ftp_cmd',\n",
    "    'ct_srv_src',\n",
    "    'ct_srv_dst',\n",
    "    'ct_dst_ltm',\n",
    "    'ct_src_ltm', \n",
    "    'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_lt', \n",
    "    'ct_dst_src_ltm',\n",
    "    'attack_cat',\n",
    "    'Label'\n",
    "\n",
    "]\n",
    "\n",
    "df.drop('sport', 1, inplace=True)#\n",
    "df.drop('dsport', 1, inplace=True)#\n",
    "df.drop('state', 1, inplace=True)#\n",
    "df.drop('dur', 1, inplace=True)#\n",
    "df.drop('Sload', 1, inplace=True)#\n",
    "df.drop('Dload', 1, inplace=True)#\n",
    "df.drop('Stime', 1, inplace=True)#\n",
    "df.drop('Ltime', 1, inplace=True)#\n",
    "df.drop('ct_src_ltm', 1, inplace=True)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tf_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
